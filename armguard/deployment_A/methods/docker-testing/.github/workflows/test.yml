name: ArmGuard Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run security scans daily at 2 AM
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - functional
          - security
          - performance
      performance_users:
        description: 'Number of users for performance test'
        required: false
        default: '50'

env:
  DOCKER_BUILDKIT: 1
  COMPOSE_DOCKER_CLI_BUILD: 1
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  # ==========================================================================
  # Build and Unit Tests
  # ==========================================================================
  build:
    name: Build and Unit Tests
    runs-on: ubuntu-latest
    outputs:
      image_tag: ${{ steps.meta.outputs.tags }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-django pytest-cov flake8 safety bandit

      - name: Lint with flake8
        run: |
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

      - name: Security check with Bandit
        run: |
          bandit -r . -x ./venv,./testing_environment -f json -o bandit-report.json || true
          bandit -r . -x ./venv,./testing_environment

      - name: Dependency security check
        run: |
          safety check --full-report || true

      - name: Run unit tests
        run: |
          python manage.py test --settings=core.settings --verbosity=2
        env:
          DJANGO_SECRET_KEY: 'test-secret-key-for-ci-only'
          DEBUG: 'False'

      - name: Upload test coverage
        uses: codecov/codecov-action@v4
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          fail_ci_if_error: false

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: testing_environment/Dockerfile
          push: ${{ github.event_name != 'pull_request' }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: bandit-report
          path: bandit-report.json
          retention-days: 30

  # ==========================================================================
  # Functional Tests
  # ==========================================================================
  functional-tests:
    name: Functional Tests
    runs-on: ubuntu-latest
    needs: build
    if: github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'functional' || github.event.inputs.test_type == ''
    
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: armguard_test
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: armguard_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-django pytest-xdist selenium requests

      - name: Start application
        run: |
          python manage.py migrate
          python manage.py collectstatic --noinput
          python manage.py runserver 0.0.0.0:8000 &
          sleep 10
        env:
          DATABASE_URL: postgres://armguard_test:test_password@localhost:5432/armguard_test
          REDIS_URL: redis://localhost:6379/0
          DJANGO_SECRET_KEY: 'test-secret-key-for-ci-only'
          DEBUG: 'False'

      - name: Set up Chrome
        uses: browser-actions/setup-chrome@latest

      - name: Set up ChromeDriver
        uses: nanasess/setup-chromedriver@v2

      - name: Run functional tests
        run: |
          cd testing_environment/functional_tests
          pytest test_functional.py test_api.py -v --tb=short --junitxml=functional-results.xml
        env:
          APP_URL: http://localhost:8000
          SELENIUM_DRIVER: chrome
          HEADLESS: 'true'

      - name: Upload functional test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: functional-test-results
          path: testing_environment/functional_tests/functional-results.xml
          retention-days: 30

      - name: Publish test results
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: Functional Tests Report
          path: testing_environment/functional_tests/functional-results.xml
          reporter: java-junit

  # ==========================================================================
  # Security Tests
  # ==========================================================================
  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    needs: build
    if: github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'security' || github.event.inputs.test_type == ''
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Start test environment
        run: |
          cd testing_environment
          docker compose up -d armguard-db armguard-redis armguard-app nginx
          sleep 30

      - name: Wait for application
        run: |
          for i in {1..30}; do
            if curl -ks https://localhost/health/ > /dev/null 2>&1; then
              echo "Application is ready"
              break
            fi
            echo "Waiting for application... ($i/30)"
            sleep 10
          done

      - name: Run OWASP ZAP Baseline Scan
        uses: zaproxy/action-baseline@v0.12.0
        with:
          target: 'https://localhost'
          rules_file_name: 'testing_environment/security_tests/zap/zap-rules.tsv'
          allow_issue_writing: false

      - name: Run OWASP ZAP Full Scan
        uses: zaproxy/action-full-scan@v0.10.0
        with:
          target: 'https://localhost'
          allow_issue_writing: false
        continue-on-error: true

      - name: Security Headers Check
        run: |
          echo "Checking security headers..."
          response=$(curl -s -I -k https://localhost/)
          
          headers_to_check=(
            "X-Frame-Options"
            "X-Content-Type-Options"
            "X-XSS-Protection"
            "Strict-Transport-Security"
            "Content-Security-Policy"
          )
          
          for header in "${headers_to_check[@]}"; do
            if echo "$response" | grep -qi "$header"; then
              echo "✓ $header present"
            else
              echo "✗ $header MISSING"
            fi
          done

      - name: SSL/TLS Configuration Check
        run: |
          echo "Checking SSL/TLS configuration..."
          # Check for SSLv3 (should fail)
          if timeout 5 openssl s_client -connect localhost:443 -ssl3 2>&1 | grep -q "handshake failure"; then
            echo "✓ SSLv3 disabled"
          fi
          
          # Check for TLS 1.2 support (should work)
          if timeout 5 openssl s_client -connect localhost:443 -tls1_2 2>&1 | grep -q "connected"; then
            echo "✓ TLS 1.2 supported"
          fi

      - name: Collect security scan results
        if: always()
        run: |
          mkdir -p security-reports
          cp zap-baseline-report.html security-reports/ || true
          cp zap-full-scan-report.html security-reports/ || true

      - name: Upload security test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-test-results
          path: security-reports/
          retention-days: 30

      - name: Stop test environment
        if: always()
        run: |
          cd testing_environment
          docker compose down -v

  # ==========================================================================
  # Performance Tests
  # ==========================================================================
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: build
    if: github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'performance' || github.event.inputs.test_type == ''
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Locust
        run: pip install locust

      - name: Start test environment
        run: |
          cd testing_environment
          docker compose up -d armguard-db armguard-redis armguard-app nginx
          sleep 30

      - name: Wait for application
        run: |
          for i in {1..30}; do
            if curl -ks https://localhost/health/ > /dev/null 2>&1; then
              echo "Application is ready"
              break
            fi
            echo "Waiting for application... ($i/30)"
            sleep 10
          done

      - name: Run performance tests
        run: |
          mkdir -p performance-results
          
          users=${{ github.event.inputs.performance_users || '50' }}
          
          locust -f testing_environment/performance_tests/locustfile.py \
            --headless \
            --host=https://localhost \
            --users=$users \
            --spawn-rate=5 \
            --run-time=5m \
            --csv=performance-results/perf \
            --html=performance-results/performance-report.html \
            --only-summary

      - name: Analyze performance results
        run: |
          echo "## Performance Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          
          if [ -f performance-results/perf_stats.csv ]; then
            tail -1 performance-results/perf_stats.csv | awk -F',' '{
              print "| Total Requests | "$3" |"
              print "| Failures | "$4" |"
              print "| Median Response | "$6"ms |"
              print "| 95th Percentile | "$8"ms |"
              print "| Requests/sec | "$10" |"
            }' >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload performance test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-test-results
          path: performance-results/
          retention-days: 30

      - name: Stop test environment
        if: always()
        run: |
          cd testing_environment
          docker compose down -v

  # ==========================================================================
  # Generate Final Report
  # ==========================================================================
  generate-report:
    name: Generate Test Report
    runs-on: ubuntu-latest
    needs: [build, functional-tests, security-tests, performance-tests]
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: test-artifacts

      - name: Generate consolidated report
        run: |
          cat > test-report.md << 'EOF'
          # ArmGuard Test Report
          
          **Generated:** $(date -u)
          **Commit:** ${{ github.sha }}
          **Branch:** ${{ github.ref_name }}
          **Triggered by:** ${{ github.event_name }}
          
          ## Test Summary
          
          | Test Suite | Status |
          |------------|--------|
          | Build & Unit Tests | ${{ needs.build.result }} |
          | Functional Tests | ${{ needs.functional-tests.result }} |
          | Security Tests | ${{ needs.security-tests.result }} |
          | Performance Tests | ${{ needs.performance-tests.result }} |
          
          ## Artifacts
          
          All detailed reports are available as workflow artifacts:
          - `bandit-report` - Static security analysis
          - `functional-test-results` - Functional test JUnit report
          - `security-test-results` - OWASP ZAP scan results
          - `performance-test-results` - Locust performance reports
          
          ## Next Steps
          
          1. Review any failed tests
          2. Address security findings by severity
          3. Optimize endpoints with high response times
          4. Fix any test regressions before merging
          
          ---
          *Generated by ArmGuard CI/CD Pipeline*
          EOF

      - name: Post report to PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('test-report.md', 'utf8');
            
            github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: report
            });

      - name: Upload final report
        uses: actions/upload-artifact@v4
        with:
          name: test-report
          path: test-report.md
          retention-days: 90

  # ==========================================================================
  # Deploy to Staging (on main branch)
  # ==========================================================================
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [functional-tests, security-tests, performance-tests]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment: staging
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Deploy to staging
        run: |
          echo "Deploying to staging environment..."
          # Add deployment commands here
          # e.g., kubectl apply, docker compose up, etc.

      - name: Run smoke tests
        run: |
          echo "Running smoke tests on staging..."
          # Add smoke test commands

      - name: Notify deployment
        run: |
          echo "Deployment to staging complete"
